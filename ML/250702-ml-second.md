# ML : Machine Learning

## 학습 방법에 따른 방법론         
**Supervised Learning 지도학습**      
* 입력 데이터(x)에 대해 어떤 결과가 정답으로 나와야 하는 학습법         
* 독립변수에 따른 종속변수가 있는 것          
* 정답이 있는 데이터를 활용해 학습시키는 방법 
    * 회귀모델        
        * 종속변수(y)가 연속형 변수인 모델     
        * e.g. 집 가격 예측, 강수량 예측       
        * 방법론 : 선형회귀모델, 상관관계 분석             
    * 분류모델           
        * 종속변수(y)가 범주형 또는 이산형 변수인 모델            
        * e.g. 스팸메일 여부        
        * 방법론 : 로지스틱 회귀모델, kNN 분류, 결정트리, 서포트 벡터머신                   
                              
**Unsupervised Learning**             
* 정답을 알려주지 않고 학습           
* 독립변수에 따른 종속변수가 없는 것             
* 입력 데이터만 가지고 패턴을 찾아내는 학습                   
    * 차원 축소               
    * 군집화                  
* 비지도 학습의 이유 : 매니폴드 가설            

### 지도학습 - 회귀   
* 회귀 : 하나 이상의 독립변수들이 종속변수에 얼마나 영향을 미치는지 추정      
* 모델 : 변수가 연속형 변수일 때                              
**선형회귀**                      
* 입력변수 개수            
    * 한 개 : 단순 회귀분석              
    * 여러 개 : 다중 회귀분석                 
* 출력변수 개수              
    * 한 개 : (단변량) 회귀분석           
    * 여러 개 : 다변량 회귀분석              
* 선형 모델과 비선형 모델                      
    * 선형 회귀분석        
    * 비선형 회귀분석             
============================================================================                        
* 단순 선형 회귀 : 직선의 기울기(a)와 y절편(b)을 조절해 데이터(점)와의 오차가 가장 작아지도록 만듦    
    * Y = ax + b            
    * 오차 : MSE 손실함수 사용            
        * MSE 손실함수 : 오차(목표값 - 출력값)의 제곱을 평균한 값            
        * y, y_hat |-> (y - y_hat)^2 ; y_hat은 직선, y는 목표값                
        * 다중선형 회귀분석 : 𝜃 = (X^TX)^-1 X^Ty               
    * 상관관계 분석                
        * 상관관계 : 두 변수 간 선형적 관계가 존재(한 변수 변화 시 다른 변수도 변화)        
        * 인과관계 : 한 변수의 변화가 원인이 되어 다른 변수를 변화시킴                 
        * 인과관계가 있는 변수는 상관관계도 있음             
        * 상관관계가 있다고 인과관계가 있는 것은 **아님**             
        * 피어슨 상관분석(선형적 상관관계) : -1 ~ 1 범위의 숫자로 상관관계 표현, 0에 가까울수록 상관관계 없는 것, 두 변수가 정규분포를 따른다는 가정 필요                         
* 회귀분석 가정                   
    * 선형성 : 독립변수와 종속변수가 선형적                 
    * 독립성 : 잔차와 독립변수 값이 독립적                       
    * 등분산성 : 잔차의 분산이 같아야 함               
    * 정규성 : 각 변수가 정규분포를 따라야 함               

### 지도학습 - 분류                            
* 이진분류 : 출력값이 두 가지로만 나오는 간단한 형태                    
* 모델 : 변수가 범주형 또는 이산형                  
* 종류 : 로지스틱 회귀분석                   
**로지스틱 회귀분석**                      
* 종속변수가 범주형 변수를 반환하지 않고 각 범주에 포함될 확률값을 반환하여 분류                  
* 로지스틱 회귀분석 알고리즘               
    * 오즈 : 성공확률이 실패확률 몇 배인지 나타내는 값             
    * 로짓변환 : 오즈에 로그값 취한 형태로 성공확률 0.5를 기준으로 대칭 형태                 
    * 시그모이드 함수 : 로짓함수와 역함수 관계                                  
* 손실함수 : 크로스엔트로피               
    * 크로스엔트로피 : y, y_hat |-> (ylogy_hat + (1-y)log(1-y_hat))               
    * 사용 이유 : 모델 예측이 틀린 경우 패널티를 부과해 학습 성능 올림, 로지스틱함수와 결합 시 미분식 간단해짐             
===============================================================================                     
* 다중분류 : 클래스가 k개인 다중분류 출력값은 i번째 원속 i번째 클래스가 정답일 확률을 나타내는 k차 벡터 형태                      
    * 원 핫 인코딩 : 정답 클래스가 i번째일 경우, i번째 원소만 1이고 나머지 값이 0인 벡터로 라벨 표현    
    * 소프트맥스 함수 : logit 벡터를 각 클래스에 대한 확률로 나타내는 벡터로 변환         
* 종류 : k-최근접 이웃 알고리즘(kNN), 의사결정트리, SVM            
--------------------------------------------------------------------                   
* k-최근접 이웃 알고리즘(kNN) : 새 input이 들어왔을 때, 학습 데이터셋에서 가장 근접한 k개 라벨을 기준으로 출력값을 결정하는 간단한 방식                  
* 의사결정나무 : 독립변수 내 대소관계나 특정 임계값과 비교하여 계층적인 판단을 적용해 최종 결과 분류    
* 랜덤포레스트               
    * 앙상블 : 랜덤한 구조의 결정트리를 여러 개 생성한 뒤 각 트리 결과를 종합해 최종 출력값 결정      
* 서포트벡터머신(SVM) : 선형분리 가능한 데이터셋에서 두 클래스를 가장 잘 분리하는 결정경계를 찾아내기 위한 방법론        
    * 데이터와 결정경계 사이의 거리인 마진값을 최대화하도록 학습             
    * 결정경계 : 이진 분류 모델에서 판단 기준인 초평면         
    * 선형분리 가능 데이터셋 : 하나의 결정경계를 통해 이진분류 데이터셋의 두 클래스가 완전히 나누어질 수 있을 때         
    * 커널 트릭 : 커널 함수를 통해 데이터를 고차원으로 변환해 선형분리가 가능해지도록 하여 SVM 적용      
### 비지도학습      
* 입력 데이터 사이의 관계를 활용      
* 별도의 방법으로 데이터 행렬 구조 분석      
* 입력값에 내재된 정보로 가상 라벨 생성      
* 자기 자신(x)을 라벨로 활용      
* 데이터의 본질 이해를 위함              
    * 매니폴드 가설 : 고차원 데이터들은 대체적으로 매우 적은 차원의 매니폴드를 이루고 있을 것    
    * 차원을 줄일 수 있다는 사실에 집중 : 차원축소, 노이즈 제거, 시각화       
    * 매니폴드 구조 : 군집화, 이상값 탐지         
**군집화**          
* 데이터로부터 패턴을 파악해 데이터를 여러 개 군집으로 나누는 것       
* 군집화와 분류의 차이     
    * 분류 : 출력값과 레이블을 비교해 목표값에 가깝도록 학습 -> 결과와 레이블 비교        
    * 군집화 : 데이터 유사성을 기반으로 분리, 데이터 사이의 거리 비교 -> 데이터 간 관계 비교     
--------------------------------------------------------------------------                  
* 차원 축소 : 고차원 데이터의 정보를 최대한 보존하면서 훨씬 적은 차원으로 표현 => PCA(주성분 분석)     
* 시각화 : 고차원 데이터를 차원축소해 그 구조를 알기 쉽게 보여줌(구조 변형 없음) => t-SNE            
* 노이즈 제거 : 차원축소 시 정보가 유실된다는 사실을 활용해 원본 데이터의 노이즈 제거      
* 군집화 : 데이터 간 유사성이나 관계를 통해 여러 개의 군집으로 나움(데이터 변형 없음)       
* 이상값 탐지 : 일반적인 input값과 다른 이상갑 찾기          
* 임베딩 : 자연어처리에서 사용하는 방법론으로 자연어 단어들을 의미적인 연산이 가능한 임베딩 공간에 맵핑    

## 차원     
* 차원 : 데이터 내 feature에 따라 고저가 나뉨     
* 고차원 데이터 필요 이유     
    * 현실은 고차원    
    * 데이터 표현 시 차원이 부족할 경우 제대로 된 분석이 어려움     
* 고차원 문제점    
    * 컴퓨터 자원의 사용량이 늘어남       
    * 차원에 따른 데이터의 sparsity 문제 -> 고차원 데이터 간 거리가 지수적으로 증가해 공간 내 데이터 밀도가 낮아져 모델 학습이 어려워짐      
    * 직관적으로 이해하기 힘듦 -> 익숙한 개념이 고차원 공간에서는 이상한 특성을 보임      
